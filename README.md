# Pretrained Models

## 学習性能改良
* [ ] BERT
* [ ] XLNet
* [ ] RoBERTa
* [ ] DeBERTa
* [ ] XLM
* [ ] BART
* [ ] ELECTRA
* [ ] T5

## モデル圧縮
* [ ] Q8BERT
  * https://arxiv.org/abs/1910.06188
  * https://github.com/IntelLabs/nlp-architect/blob/master/nlp_architect/models/transformers/quantized_bert.py
* [ ] DistilBERT
* [ ] ALBERT
* [ ] CompressingBERT
* [ ] BERT-of-Theseus

## 下流タスク
* [ ] GPT-2
* [ ] GPT-3

## 多言語
* [ ] mBART
* [ ] mT5

## マルチモーダル
* [ ] DALL-E
* [ ] VilBERT
* [ ] VideoBERT
* [ ] SpeechBERT

# References
* https://elyza-inc.hatenablog.com/entry/2021/03/25/160727
* https://scrapbox.io/miyawaki-nlp/事前学習済みモデル
